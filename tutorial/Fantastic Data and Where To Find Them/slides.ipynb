{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<script type=\"text/javascript\">\n",
    "\n",
    "$(document).ready(function(){\n",
    "\n",
    "    Reveal.configure({\n",
    "        transition: 'convex' // none/fade/slide/convex/concave/zoom\n",
    "    })\n",
    "});    \n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Hi.</center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<center>\n",
    "<p>Everything for this workshop is available online here \n",
    "<p><a href=\"https://github.com/nd1/pycon_2017\">https://github.com/nd1/pycon_2017</a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Fantastic Data and Where To Find Them: An introduction to APIs, RSS, and Scraping</center>\n",
    "\n",
    "\n",
    "<br/>\n",
    "<b>Nicole Donnelly, District Data Labs</b>\n",
    "\n",
    "[PyCon 2017](https://us.pycon.org/2017/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Hello! My name is Nicole Donnelly.\n",
    "\n",
    "I am going to present a workshop of data collection, specifically using python to collect data from RSS, APIs, and to build a basic web scraper.\n",
    "\n",
    "To that end..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center> Who Are You?  </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center> How many people know python?  </center>\n",
    "<center> How many people have used RSS? </center>\n",
    "<center> How many people have used APIs? </center>\n",
    "<center> How many people have tried web scraping? </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center> Who Are We? </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img align=\"middle\" src=\"./img/ddl.png\" alt=\"ddl\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Data Science Research Institute</center>\n",
    "<center>Data Product Incubator</center>\n",
    "<center>Open Source Collaborative</center>\n",
    "<center>Community</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "District Data Labs is a data science research institute, data product incubator, and open source collaborative where people from diverse backgrounds come together to work on interesting projects, push themselves beyond their current capabilities, and help each other become more successful data scientists. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center> Tony Ojeda </center>\n",
    "\n",
    "<center><img align=\"middle\" src=\"./img/tony.jpeg\" alt=\"tony\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center> Data Scientist </center>\n",
    "<center> Author </center>\n",
    "<center> Entrepreneur </center>\n",
    "<center> Founder, District Data Labs </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center> Will Voorhees </center>\n",
    "\n",
    "<center><img align=\"middle\" src=\"./img/will.jpeg\" alt=\"will\"></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Developer</center>\n",
    "<center>Security Fan</center>\n",
    "<center>YouTube Aficionado</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center> Nicole Donnelly </center>\n",
    "\n",
    "<center><img align=\"middle\" src=\"./img/nicole.jpeg\" alt=\"nicole\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center> DC Data Analyst </center>\n",
    "<center> Data Science Faculty </center>\n",
    "<center> Recovering Consultant </center>\n",
    "<center> Fiber Alchemist </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In my past life as a consultant, I spent a lot of time with data. I worked in computer forensics and electronic discovery. I collected data, inventoried it, organized it, gave it context, analyzed it, reconstructed it, reported on it, and put it in useful formats for people to review and use. I was frequently asked to help inventory and organize data for other projects because I am good at it and I enjoy it.\n",
    "\n",
    "I decided to make a career change to data science. I completed the professional certificate in data science at Georgetown, became the TA, and joined the faculty. I am also on the faculty at District Data Labs. I completed the Data Science Immersive at Genral Assembly. And now I work for the city in the Office of the Chief Technology Officer where I am detailed to the Office of Unified Communication to work on their data analysis and projects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Agenda</center>\n",
    "### APIs, an introduction\n",
    "* What are APIs?\n",
    "* Where are APIs?\n",
    "* How do I access APIs?\n",
    "* What is the API giving me?\n",
    "\n",
    "### APIs, hands-on\n",
    "* RSS\n",
    "* REST APIs with Authentication\n",
    "* Scripting API Calls\n",
    "\n",
    "### Web scraping, an introduction\n",
    "* What is web scraping?\n",
    "* When should I web scrape?\n",
    "* How do I web scrape?\n",
    "\n",
    "### Web scraping, hands-on \n",
    "* BeautifulSoup, an overview\n",
    "* Web scraping, downloading data example\n",
    "* Web scraping, try it out\n",
    "\n",
    "### Baleen, a case study\n",
    "* How can we operationalization data collection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "I am going to assume people can navigate from the command line, have python installed, and have some experience with python.\n",
    "\n",
    "I am also going to assume people have not tried using APIs.\n",
    "\n",
    "I created some notebooks so we can go through some of the python and you can see what it is doing.\n",
    "\n",
    "I have also created some scripts you can run when you are ready. Feel free to modify them to suit your needs. Or, if you have some experience with all of this, feel free to play around with those instead of the notebooks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>What are APIs?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Application Programming Interface</center>\n",
    "\n",
    "\n",
    "“In the simplest terms, APIs are sets of requirements that govern how one application can talk to another.”\n",
    "\n",
    "\n",
    "\n",
    "“APIs are what make it possible to move information between programs....”\n",
    "\n",
    "\n",
    "\n",
    "*Source: http://readwrite.com/2013/09/19/api-defined*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* APIs make data collection easier! \n",
    "* Well, until they don't... limited data, rate limiting, financial charges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Where are APIs?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img align=\"middle\" src=\"./img/APIs.png\" alt=\"apis\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Everywhere.\n",
    "\n",
    "Copy/ paste is possible because of APIs.\n",
    "\n",
    "Common API examples:\n",
    "\n",
    "* Icons to share an article from a website\n",
    "* Disqus comment system\n",
    "* RSS readers (rss is an api)(Really Simple Syndication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>What is RSS?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>RSS</center>\n",
    "\n",
    "\n",
    "“RSS is an XML-based vocabulary for distributing Web content in opt-in feeds. Feeds allow the user to have new content delivered to a computer or mobile device as soon as it is published. ”\n",
    "\n",
    "\n",
    "\n",
    "*Source: http://searchwindevelopment.techtarget.com/definition/RSS*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are different versions - RSS 1.0, RSS 2.0, ATOM\n",
    "* RSS 1.0: RDF (Resource Description Framework) Site Summary\n",
    "* RSS 2.0: Really Simple Syndication, based on RSS 0.91\n",
    "* ATOM: functionally similar, but is a formal specification from Internet Engineering Task Force (IETF)\n",
    "\n",
    "*Additional information on RSS: https://www.mnot.net/rss/tutorial/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[iTunes is an RSS aggregator for podcasts.](http://itunespartner.apple.com/en/podcasts/faq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "RSS is defined as an acrony mulitple ways: Rich Site Summary, RDF (Resource Description Framework) Site Summary, Really Simple Syndication. It is an XML based content distribution format commonly used for blog and news data. We can use it to transfer and collect data. RSS retrieval can also be operationalized to create a corpus for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>Is RSS an API?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\"RSS is in some sense a specific API in its own right – with specific semantics for calls (the “latest” X items on this topic) and standard formats for the data being returned. This makes the content from this API accessible to many thousands of reader implementations.\"\n",
    "\n",
    "*Source: https://www.3scale.net/2012/09/on-apis-and-rss/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>How do I access RSS and APIs?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Context: I will focus on RSS and REST APIs with JSON. \n",
    "\n",
    "Read more about API types [here](https://ffeathers.wordpress.com/2014/02/16/api-types/)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## <center>RSS</center>\n",
    "[http://dvd.netflix.com/NewReleasesRSS](http://dvd.netflix.com/NewReleasesRSS)\n",
    "```xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n",
    "\n",
    "<rss version=\"2.0\" xmlns:atom=\"http://www.w3.org/2005/Atom\" >\n",
    "  <channel >\n",
    "    <title>New Releases This Week</title>\n",
    "    <ttl>10080</ttl>\n",
    "    <link>http://dvd.netflix.com</link>\n",
    "    <description>New movies at Netflix this week</description>\n",
    "    <language>en-us</language>\n",
    "    <cf:treatAs xmlns:cf=\"http://www.microsoft.com/schemas/rss/core/2005\">list</cf:treatAs>\n",
    "    <atom:link href=\"http://dvd.netflix.com/NewReleasesRSS\" rel=\"self\" type=\"application/rss+xml\"/>\n",
    "    <item>\n",
    "      <title>Bakery in Brooklyn</title>\n",
    "      <link>https://dvd.netflix.com/Movie/Bakery-in-Brooklyn/80152426</link>\n",
    "      <guid isPermaLink=\"true\">https://dvd.netflix.com/Movie/Bakery-in-Brooklyn/80152426</guid>\n",
    "      <description>&lt;a href=&quot;https://dvd.netflix.com/Movie/Bakery-in-Brooklyn/80152426&quot;&gt;&lt;img src=&quot;//secure.netflix.com/us/boxshots/small/80152426.jpg&quot;/&gt;&lt;/a&gt;&lt;br&gt;Vivien and Chloe have just inherited their Aunt's bakery, a boulangerie that has been a cornerstone of the neighborhood for years. Chloe wants a new image and product, while Vivien wants to make sure nothing changes. Their clash of ideas leads to a peculiar solution, they split the shop in half. But Vivien and Chloe will have to learn to overcome their differences in order to save the bakery and everything that truly matters in their lives.</description>\n",
    "    </item>```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Context: I will focus on RSS and REST APIs with JSON. \n",
    "\n",
    "Read more about API types [here](https://ffeathers.wordpress.com/2014/02/16/api-types/)\n",
    "\n",
    "\n",
    "## <center>API</center>\n",
    "[http://dev.markitondemand.com/Api/Quote/json?symbol=AAPL](http://dev.markitondemand.com/Api/Quote/json?symbol=AAPL)\n",
    "\n",
    "```json\n",
    "    {\"Data\":{\"Status\":\"SUCCESS\",\"Name\":\"Apple Inc\",\"Symbol\":\"AAPL\",\"LastPrice\":117.12,\"Change\":-0.349999999999994,\"ChangePercent\":-0.297948412360598,\"Timestamp\":\"Wed Oct 19 00:00:00 UTC-04:00 2016\",\"MarketCap\":631094444160,\"Volume\":20034594,\"ChangeYTD\":105.26,\"ChangePercentYTD\":11.2673380201406,\"High\":117.76,\"Low\":113.8,\"Open\":117.25}}\n",
    "```\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "It is as easy as constructing the correct url!\n",
    "\n",
    "REST was influenced by HTTP so is almost always implemented that way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>The Anatomy of a URL</center>\n",
    "\n",
    "<center>REST was influence by HTTP and API resources are accessed by constructing URLs</center>\n",
    "\n",
    "<p>\n",
    "\n",
    "<center><img align=\"middle\" src=\"./img/url_anatomy.png\" alt=\"url_anatomy\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img align=\"middle\" src=\"./img/restful.png\" alt=\"request_response\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### HTTP Status Codes \n",
    "\n",
    " - **1xx** - Informational\n",
    " - **2xx** - Success\n",
    " - **3xx** - Redirection\n",
    " - **4xx** - Client Error\n",
    " - **5xx** - Server Error\n",
    " \n",
    "A complete list can be found here: [HTTP Statuses](https://httpstatuses.com/). \n",
    "\n",
    "A response of `200 Ok` means that the request was successful. Other common status codes include:\n",
    "\n",
    "- **404 Not Found**: the requested path does not exist on the server \n",
    "- **500 Server Error**: something went very wrong on the server \n",
    "- **301 Redirect**: the resource has moved to a different URL \n",
    "- **403 Forbidden**: the resource requires authentication "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "http://api.dp.la/v2/items?api_key=<font color=\"red\">0123456789</font>&q=goats+AND+cats\n",
    "```json\n",
    "    {\"count\":29,\n",
    "    \"start\":0,\n",
    "    \"limit\":10,\n",
    "    \"docs\":[{\"@context\":\"http://dp.la/api/items/context\",\"isShownAt\":\"http://cdm16795.contentdm.oclc.org/cdm/ref/collection/divtour/id/88\",\"dataProvider\":\"Missouri State Archives through Missouri Digital Heritage\",\"@type\":\"ore:Aggregation\",\"provider\":{\"@id\":\"http://dp.la/api/contributor/missouri-hub\",\"name\":\"Missouri Hub\"},\"hasView\":{\"@id\":\"http://cdm16795.contentdm.oclc.org/cdm/ref/collection/divtour/id/88\"},\"object\":\"http://data.mohistory.org/files/thumbnails/cdm16795_contentdm_oclc_org568ad334407e0.jpg\",\"ingestionSequence\":12,\"id\":\"9e05f398ca95f9bbfd733e6d3493fd74\",\"ingestDate\":\"2016-10-11T13:21:48.399681Z\",\"_rev\":\"7-6bee4d18708d1d16efceeea1e061b316\",\"aggregatedCHO\":\"#sourceResource\",\"_id\":\"missouri--urn:data.mohistory.org:mdh_all:oai:cdm16795.contentdm.oclc.org:divtour/88\",\"sourceResource\":{\"title\":[\"Alabama Big Cats Safari Adventure\"],\"description\":[\"Children bottle feeding goats\"],\"subject\":[{\"name\":\"Transparencies, Slides\"},{\"name\":\"Tourist Destination\"}],\"rights\":[\"Copyright is in the public domain. Items reproduced for publication should carry the credit line: Courtesy of the Missouri State Archives.\"],\"relation\":[\"Division of Tourism Photograph Collection\"],\"language\":[{\"iso639_3\":\"eng\",\"name\":\"English\"}],\"format\":\"Image\",\"collection\":{\"id\":\"594a2b3666ab0c55245f6640555554cd\",\"description\":\"\",\"title\":\"Mdh_divtour\",\"@id\":\"http://dp.la/api/collections/594a2b3666ab0c55245f6640555554cd\"},\"stateLocatedIn\":[{\"name\":\"Missouri\"}],\"@id\":\"http://dp.la/api/items/9e05f398ca95f9bbfd733e6d3493fd74#sourceResource\",\"identifier\":[\"001_070\",\"http://cdm16795.contentdm.oclc.org/cdm/ref/collection/divtour/id/88\"],\"creator\":[\"GD\"]},\"admin\":{\"validation_message\":null,\"sourceResource\":{\"title\":\"Alabama Big Cats Safari Adventure\"},\"valid_after_enrich\":true},\"ingestType\":\"item\",\"@id\":\"http://dp.la/api/items/9e05f398ca95f9bbfd733e6d3493fd74\",\"originalRecord\":{\"id\":\"urn:data.mohistory.org:mdh_all:oai:cdm16795.contentdm.oclc.org:divtour/88\",\"provider\":{\"@id\":\"http://dp.la/api/contributor/missouri-hub\",\"name\":\"Missouri Hub\"},\"collection\":{\"id\":\"594a2b3666ab0c55245f6640555554cd\",\"description\":\"\",\"title\":\"mdh_divtour\",\"@id\":\"http://dp.la/api/collections/594a2b3666ab0c55245f6640555554cd\"},\"header\":{\"expirationdatetime\":\"2016-10-08T17:04:17Z\",\"datestamp\":\"2016-10-04T13:19:05Z\",\"identifier\":\"urn:data.mohistory.org:mdh_all:oai:cdm16795.contentdm.oclc.org:divtour/88\",\"setSpec\":\"mdh_divtour\"},\"metadata\":{\"mods\":{\"accessCondition\":\"Copyright is in the public domain. Items reproduced for publication should carry the credit line: Courtesy of the Missouri State Archives.\",\"location\":{\"url\":[{\"#text\":\"http://cdm16795.contentdm.oclc.org/cdm/ref/collection/divtour/id/88\",\"access\":\"object in context\"},{\"#text\":\"http://data.mohistory.org/files/thumbnails/cdm16795_contentdm_oclc_org568ad334407e0.jpg\",\"access\":\"preview\"}]},\"subject\":[{\"topic\":\"Transparencies, Slides\"},{\"topic\":\"Tourist Destination\"}],\"name\":{\"namePart\":\"GD\",\"role\":{\"roleTerm\":\"creator\"}},\"relatedItem\":{\"titleInfo\":{\"title\":\"Division of Tourism Photograph Collection\"}},\"physicalDescription\":{\"note\":\"Image\"},\"xmlns\":\"http://www.loc.gov/mods/v3\",\"language\":{\"languageTerm\":\"eng\"},\"titleInfo\":{\"title\":\"Alabama Big Cats Safari Adventure\"},\"identifier\":[\"001_070\",\"http://cdm16795.contentdm.oclc.org/cdm/ref/collection/divtour/id/88\"],\"note\":[\"Children bottle feeding goats\",{\"#text\":\"Missouri State Archives through Missouri Digital Heritage\",\"type\":\"ownership\"}]}}},\"score\":4.534843}, ... \n",
    "    \"facets\":[]}```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Sometimes you will need an API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center>A Brief Aside: How should I store my API keys?</center>\n",
    "\n",
    "I like the recommendations [here](http://blog.revolutionanalytics.com/2015/11/how-to-store-and-use-authentication-details-with-r.html) and [here](http://www.blacktechdiva.com/hide-api-keys/). Pick the method that works best given your situation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this tutorial, you will see info in the notebook on creating a json file with your keys. If you follow a naming convention for your API key files, you can easily add them to your .gitignore file to avoid inadvertantly uploading them to git."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center>What is the API giving me?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[http://dev.markitondemand.com/Api/Quote/json?symbol=AAPL](http://dev.markitondemand.com/Api/Quote/json?symbol=AAPL)\n",
    "\n",
    "```json\n",
    "{\"Data\":{\"Status\":\"SUCCESS\",\"Name\":\"Apple Inc\",\"Symbol\":\"AAPL\",\"LastPrice\":117.06,\"Change\":-0.0600000000000023,\"ChangePercent\":-0.0512295081967233,\"Timestamp\":\"Thu Oct 20 00:00:00 UTC-04:00 2016\",\"MarketCap\":630771137580,\"Volume\":24059570,\"ChangeYTD\":105.26,\"ChangePercentYTD\":11.2103363100893,\"High\":117.38,\"Low\":116.33,\"Open\":116.86}}```\n",
    "\n",
    "\n",
    "\n",
    "[http://dev.markitondemand.com/Api/Quote/xml?symbol=AAPL](http://dev.markitondemand.com/Api/Quote/xml?symbol=AAPL)\n",
    "```xml\n",
    "<QuoteApiModel>\n",
    "<Data>\n",
    "<Status>SUCCESS</Status>\n",
    "<Name>Apple Inc</Name>\n",
    "<Symbol>AAPL</Symbol>\n",
    "<LastPrice>117.06</LastPrice>\n",
    "<Change>-0.06</Change>\n",
    "<ChangePercent>-0.0512295082</ChangePercent>\n",
    "<Timestamp>Thu Oct 20 00:00:00 UTC-04:00 2016</Timestamp>\n",
    "<MarketCap>630771137580</MarketCap>\n",
    "<Volume>24059570</Volume>\n",
    "<ChangeYTD>105.26</ChangeYTD>\n",
    "<ChangePercentYTD>11.2103363101</ChangePercentYTD>\n",
    "<High>117.38</High>\n",
    "<Low>116.33</Low>\n",
    "<Open>116.86</Open>\n",
    "</Data>\n",
    "</QuoteApiModel>```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "APIs return serialized data.\n",
    "\n",
    "JSON stands for \"JavaScript Object Notation\", and has become a universal standard for serializing native data structures for transmission. It is light-weight, easy to read, and quick to parse. It is easy to use in python with the json library.\n",
    "\n",
    "XML stands for \"eXtensible Markup Language\", and is the granddaddy of serialized data formats (itself based on HTML). XML is fat, ugly, and cumbersome to parse. However, it remains a major format due to its legacy usage across the web. Most people favor using a JSON API, if available. There are xml libraries for python as well: lxml, etree, sax, minidom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Workshop</center>\n",
    "\n",
    "### Please download the notebooks to try this out.\n",
    "* RSS (notebook)\n",
    "* REST APIs with Authentication (notebook)\n",
    "* Scripting API Calls (script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Summary</center>\n",
    "\n",
    "* APIs provide access to data\n",
    "* APIs return serialized data\n",
    "* APIs are everywhere \n",
    "* RSS is an API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>What is web scraping?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Web Scraping goes by many names - Screen Scraping, Web Data Extraction, Web Harvesting\n",
    "* It is the process of directly accessing webpages to copy data when APIs are not provided\n",
    "* It automates the process of copying the data and saving it locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We will not be crawling in this tutorail.\n",
    "\n",
    "Crawling (done by crawlers or spiders) involves the traversal of a website's link network, while saving or indexing all the pages in that network.\n",
    "\n",
    "Scraping is done with an explicit purpose of extracting specific information from a page, while crawling is done in order to obtain information about link networks within and between websites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* It can be a vioation of terms of service\n",
    "* It can have legal repercussions\n",
    "* Even when it isn't a violation/ illegal, it is ethically ambiguous\n",
    "* It can look like a DOS and can overwhelm resources making them unavailable to others\n",
    "* Scraping needs to be customized for each site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>When should I web scrape?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## \"IF YOU NEED A SCRAPER, YOU HAVE A DATA PROBLEM.\" \n",
    "\n",
    "### [David Eads](https://twitter.com/eads), [PyData DC, 10/9/16](https://www.youtube.com/watch?v=ECM-a8Y_OjY)\n",
    "#### [Useful Scraping Techniques](http://blog.apps.npr.org/2016/06/17/scraping-tips.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "When I first tried web scraping, a friend sent me a link to a blog post that David Eads wrote about a scraper he built that is pretty amazing. He also gave a good talk about scraping at the last PyData DC. He works at NPR on the visuals team.\n",
    "\n",
    "I am borrowing, with permission, a couple of his slides.\n",
    "\n",
    "\"Fundamentally, a scraper means something isn’t right in the world. There’s a database out there, somewhere, powering a website, but you only have access to the website. \n",
    "\n",
    "But the good data… well, if it was easy to get, it probably wouldn’t be the good shit.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# NEVER SCRAPE UNLESS YOU...\n",
    "\n",
    "* **have no other way of liberating the data**\n",
    "* **budget appropriately**\n",
    "* **consider the ethical ramifications (do some googling and read about the ethics)**\n",
    "* **read terms of service and do your legal research**\n",
    "* **talk to a lawyer (if you possibly can)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "I am not a lawyer, and I don't play one on tv.\n",
    "\n",
    "If time allows, try FOIA. \n",
    "Scraping can turn into a denial of service attack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img align=\"middle\" src=\"./img/mimitw.jpg\" alt=\"meme\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "My current nemesis is the DOH web site.\n",
    "\n",
    "DC makes DOH inspection reports, but the data is hosted by a 3rd party. It is not \"open\" and there is no public facing API.\n",
    "\n",
    "I was blocked and had to work around it. (selenium to download pages, long delays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>How do I web scrape?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center>robots.txt</center>\n",
    "\n",
    "The robots exclusion standard, also known as the robots exclusion protocol or simply robots.txt, is a standard used by websites to communicate with web crawlers and other web robots. The standard specifies how to inform the web robot about which areas of the website should not be processed or scanned. Robots are often used by search engines to categorize web sites. Not all robots cooperate with the standard; email harvesters, spambots, malware, and robots that scan for security vulnerabilities may even start with the portions of the website where they have been told to stay out. The standard is different from, but can be used in conjunction with, Sitemaps, a robot inclusion standard for websites.\n",
    "\n",
    "Source: [Wikipedia](https://en.wikipedia.org/wiki/Robots_exclusion_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```User-agent: *\n",
    "Disallow: /cgi-bin/\n",
    "Disallow: /tmp/\n",
    "Disallow: /~joe/```\n",
    "\n",
    "[About /robots.txt](http://www.robotstxt.org/robotstxt.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are a lot of libraries availble in Python to help with this task:\n",
    "\n",
    "* urllib2 - module for processing urls, expanded in [Python 3](https://docs.python.org/3/howto/urllib2.html)\n",
    "* [requests](http://docs.python-requests.org/en/latest/index.html) - improved upon urllib/ urllib2 \n",
    "* [lxml](http://lxml.de/) - extensive library for parsing XML and HTML, some people find it a little more difficult to use initially\n",
    "* [beautifulsoup4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) - library for pulling data out of HTML and XML files\n",
    "\n",
    "There is usually more than one way to do things in Python. What you end up with tends to be what you learn first and/ or what you are most comfortable with.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* [Scrapy](https://doc.scrapy.org/en/latest/intro/overview.html) - application framework for scraping\n",
    "* [Nutch](https://wiki.apache.org/nutch/) - extensible and scalable open source web crawler software project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The tutorial portions will not cover scrapy and nutch, but I thought they were worth mentioning. \n",
    "\n",
    "Scrapy was originally designed for web scraping, but it can also be used to extract data using APIs (such as Amazon Associates Web Services) or as a general purpose web crawler. The docs have a tutorial you can walk though.\n",
    "\n",
    "Nutch is comprised of two codebases, namely:\n",
    "\n",
    "Nutch 1.x: A well matured, production ready crawler. 1.x enables fine grained configuration, relying on Apache Hadoop data structures, which are great for batch processing.\n",
    "Nutch 2.x: An emerging alternative taking direct inspiration from 1.x, but which differs in one key area; storage is abstracted away from any specific underlying data store by using Apache Gora for handling object to persistent mappings. This means we can implement an extremely flexibile model/stack for storing everything (fetch time, status, content, parsed text, outlinks, inlinks, etc.) into a number of NoSQL storage solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Expect a fair bit of trial and error. Web scraping is not straightforward and not for the faint of heart. It is a last resort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Summary</center>\n",
    "\n",
    "* Web scraping is hard\n",
    "* Web scraping can have legal and ethical implications\n",
    "* Try to respect robots.txt\n",
    "* Web scraping can get you some good data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> <a href=\"https://github.com/DistrictDataLabs/baleen\">Baleen</a> </center>\n",
    "<center><img align=\"middle\" src=\"./img/space_whale_by_hbitik-d4736q3.jpg\" alt=\"baleen\"></center>\n",
    "\n",
    "[\"Space Whale\"](http://hbitik.deviantart.com/art/Space-Whale-253770699) by [hbitik](http://hbitik.deviantart.com/) is licensed under [CC BY-NC-ND 3.0](https://creativecommons.org/licenses/by-nc-nd/3.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Python application\n",
    "* Reads RSS feeds\n",
    "* Stores pages in Mongo\n",
    "* Web frontend for status/logs\n",
    "* We'll be at the sprints\n",
    "* Easy to understand project - good open source gateway "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Interested in More from District Data Labs?</center>\n",
    "\n",
    "### DDL PyCon Schedule\n",
    "\n",
    "#### Talks\n",
    "\n",
    "Friday 4:30 p.m.–5 p.m.\n",
    "<p>[Building A Gigaword Corpus: Lessons on Data Ingestion, Management, and Processing for NLP](https://us.pycon.org/2017/schedule/presentation/297/)\n",
    "\n",
    "Saturday 2:35 p.m.–3:05 p.m.\n",
    "<p>[Human-Machine Collaboration for Improved Analytical Processes](https://us.pycon.org/2017/schedule/presentation/560/)\n",
    "\n",
    "#### Posters\n",
    "\n",
    "Sunday Morning, Expo Hall\n",
    "<p>[On the Hour Data Ingestion from the Web to a Mongo Database](https://us.pycon.org/2017/schedule/presentation/324/)\n",
    "<p>[Model Management Systems: Scikit-Learn and Django](https://us.pycon.org/2017/schedule/presentation/318/)\n",
    "<p>[Yellowbrick: Steering Scikit-Learn with Visual Transformers](https://us.pycon.org/2017/schedule/presentation/308/)\n",
    "<p>[A Framework for Exploratory Data Analysis with Python](https://us.pycon.org/2017/schedule/presentation/490/)\n",
    "\n",
    "#### Development Sprints\n",
    "[What's a sprint?](https://us.pycon.org/2017/community/sprints/)\n",
    "\n",
    "<p>[Baleen](https://github.com/DistrictDataLabs/baleen)\n",
    "<p>[Yellowbrick](https://github.com/DistrictDataLabs/yellowbrick)\n",
    "<p>[Cultivar](https://github.com/DistrictDataLabs/cultivar)\n",
    "<p>[Partisan Discourse](https://github.com/DistrictDataLabs/partisan-discourse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Stay in touch!</center>\n",
    "\n",
    "#### Nicole\n",
    "* Twitter: [@NicoleADonnelly](https://twitter.com/NicoleADonnelly)\n",
    "* GitHub: [nd1](https://github.com/nd1)\n",
    "* LinkedIn: [nicoleadonnelly](https://www.linkedin.com/in/nicoleadonnelly)\n",
    "* Email: [Nicole Donnelly](mailto:ndonnelly@districtdatalabs.com)\n",
    "\n",
    "#### Tony\n",
    "* Twitter: [@TonyOjeda3](https://twitter.com/tonyojeda3)\n",
    "* GitHub: [ojedatony1616](https://github.com/ojedatony1616)\n",
    "* LinkedIn: [tonyojeda](https://www.linkedin.com/in/tonyojeda)\n",
    "* Email: [Tony Ojeda](mailto:tojeda@districtdatalabs.com)\n",
    "\n",
    "#### Will\n",
    "* Twitter: [@will2041](https://twitter.com/will2041)\n",
    "* GitHub: [will2041](https://github.com/will2041)\n",
    "* LinkedIn: [willvoorhees](https://www.linkedin.com/in/willvoorhees)\n",
    "* Email: [Will Voorhees](mailto:wvoorhees@districtdatalabs.com)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
